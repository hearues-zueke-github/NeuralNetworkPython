#! /usr/bin/python2.7# 2015.07.13# ipython# klassen erstellen# stochastic gratienten descent (abstieg)# batch GD# use python 2.7!# 2015.10.12# - target und output vergleichen# - biases und weights ins network speichern# - inputs und targets entsprechend mit bgd, sgd aufrufen# - sgd richtig machen!- # - graph mit targets# - nicht notwendig eigene klassen! (auszer ich will es persoenlich so machen)# - MNIST Datensatz \#                    }- probieren, epochen werden lange brauchen!# - Auto Encoder    /import matplotlib# matplotlib.use("Agg")import sysfrom sys import stdoutstdoutw = stdout.writeimport osimport mathimport timeimport numpy as npimport Tkinter as tkimport matplotlib.pyplot as pltimport multiprocessing as mpfrom random import randintfrom random import uniformfrom copy import deepcopyfrom operator import itemgetterimport decimalfrom decimal import Decimal as dec# import collections# compare = lambda x, y: collections.Counter(x) == collections.Counter(y)# For colorful output in Terminalimport coloramafrom colorama import Fore, Back, Styleclass Test(object):    def __init__(self, attr1):        self.attr1 = attr1    # def __init__    def __str__(self):        return str(self.__dict__)    # def __str__    def __eq__(self, other):         return self.__dict__ == other.__dict__    # def __eq__# class Test# For clearing the Terminaldef cls():    os.system('cls' if os.name == 'nt' else 'clear')# Helpful for getting the max depth in a listdepth = lambda L: isinstance(L, list) and max(map(depth, L))+1decimal.getcontext().prec = 10np.set_printoptions(precision=10)__author__ = "haris"__date__ = "$Feb 22, 2016 23:32:35 PM$"## Class for Network## Class for Weights## Class for Biasesclass NeuralNetwork(Exception):    error_not_init = "Please, initial first the matrices!\nUse e.g. init_random() or other functions"    error_network_not_better = "This network is not better than the saved one!"    error_network_not_saved = "Cannot load network! No Network saved!"    def __init__(self, inputs = 1, outputs = 1, targets = 0, is_decimal = False):        self.inputs_amount = inputs        self.outputs_amount = outputs        self.hidden = []        self.matrix_init = False        self.weights_init = False        self.targets_init = False        self.neuron_list = [inputs, outputs]        self.weights = []        self.biases = []                # TODO: add a functionality for plotting and add the error plots in an other figure,!        # To prevent collision with the output-target plot        self.error_list = []        self.last_error = 1.0        self.is_prev_error_there = False        self.prev_error = 0        self.max_etha = 10.**-1        self.min_etha = 10.**-5        self.adapt_learn_pos = 1.05        self.adapt_learn_neg = 0.8        self.hidden_function = "sigmoid"        self.type_learning = "sgd"        self.backprop_style = ""        self.is_sgd = True        self.is_adaptive = True        self.is_no_random = False        self.network_type = "regression"        self.targets_amount = targets        if (targets > 0):            self.targets_amount_set = True        else:            self.targets_amount_set = False        # if        self.io_targets = []        self.is_decimal = is_decimal        str_num = "0.005"        if is_decimal:            self.learning_rate = dec(str_num)        else:            self.learning_rate = float(str_num)        # if        self.is_show_plot = True        # for saving temporarly the last best network config! (with inputs, and targets!)        self.is_best_network_saved = False        self.last_best_network_default = {"neuron_list": [],                                          "biases": [],                                          "weights": [],                                          "inputs": [],                                          "targets": [],                                          "learning_rate": float(str_num),                                          "lowest_error": 1.0}        self.last_best_network = deepcopy(self.last_best_network_default)    def scalar_matrix_multiplication(self, scalar, matrix):        scalar_matrix = [[float(scalar) for i2 in range(0, len(list(matrix[0])))] for i1 in range(0, len(list(matrix)))]        scalar_matrix = np.array(scalar_matrix)        return scalar_matrix * matrix    def scalar_matrix_multiplication_decimal(self, scalar, matrix):        scalar_matrix = [[dec(scalar) for i2 in range(0, len(list(matrix[0])))] for i1 in range(0, len(list(matrix)))]        scalar_matrix = np.array(scalar_matrix)        return scalar_matrix * matrix    def sigmoid(self, nparray): return 1. / (1 + np.exp(-nparray))    def sigmoid_invert(self, nparray): return self.sigmoid(nparray) * (1 - self.sigmoid(nparray))    def tanh(self, nparray): return np.tanh(nparray)    def tanh_invert(self, nparray): return (1 - np.tanh(nparray)**2)    def fsigmoid_dec(self, a) : return (dec("1") / (dec("1") + dec("2.718281828459045") ** (-a)))    def fsigmoidi_dec(self, a) : return (self.fsigmoid_dec(a) * self.fsigmoid_dec(dec("1") - a))    def ftanh_dec(self, a) : return (dec("2") / (dec("1") + dec("2.718281828459045") ** (-2 * a)))    def ftanhi_dec(self, a) : return (dec("1") - self.ftanh_dec(a) ** 2)    def sigmoid_decimal(self, vector): return np.vectorize(lambda x: self.fsigmoid_dec(x))(vector)    def sigmoid_invert_decimal(self, vector): return np.vectorize(lambda x: self.fsigmoidi_dec(x))(vector)    def tanh_decimal(self, vector): return np.vectorize(lambda x: self.ftanh_dec(x))(vector)    def tanh_invert_decimal(self, vector): return np.vectorize(lambda x: self.ftanhi_dec(x))(vector)    def error_many_rmse(self, predicts, targets):        return np.sqrt(np.mean(np.sum((predicts-targets)**2, axis=1)))    def error_many_cecf(self, predicts, targets):        return np.sum(np.vectorize(lambda targ, pred: -np.log(1-pred) if targ == 0 else -np.log(pred))(targets, predicts))    def mean_square_error_separate(self, predicts, targets):        return np.mean((predicts-targets)**2, axis=1)    def square_error_decimal(self, v1, v2):        square_error = dec("0.0")        for n1, n2 in zip(v1.transpose().tolist()[0], v2.transpose().tolist()[0]):            square_error += (dec(n1) - dec(n2))**2        return square_error    def square_error_many_decimal(self, targets_calc, targets):        square_error = dec("0.0")        for tc, t in zip(targets_calc, targets):            square_error += self.square_error_decimal(tc, t)        return square_error    def compare_two_networks(self, network1, network2):        nl1, bs1, ws1 = network1[0], network1[1], network1[2]        nl2, bs2, ws2 = network2[0], network2[1], network2[2]        # print("nl1 = "+str(nl1))        # print("nl2 = "+str(nl2))        if not (Test(nl1) == Test(nl2)):            return False        # if        for (b1, b2) in zip(bs1, bs2):            for (bl1, bl2) in zip(b1, b2):                for (bll1, bll2) in zip(bl1, bl2):                    if not (Test(bll1) == Test(bll2)):                        return False                    # if                # for            # for        # for        for (w1, w2) in zip(ws1, ws2):            for (wl1, wl2) in zip(w1, w2):                for (wll1, wll2) in zip(wl1, wl2):                    if not (Test(wll1) == Test(wll2)):                        return False                    # if                # for            # for        # for        return True    # def compare_two_networks    def stats(self):        print("inputs = " + str(self.inputs_amount))        print("outputs = " + str(self.outputs_amount))        if len(self.hidden) == 0:            print("No hidden layer")        else:            print(str(len(self.hidden)) + " layers set with " + str(self.hidden) + " neurons")        print("Neuronlist: "+str(self.neuron_list))        if self.matrix_init == True:            print("Matrix is initialized:")            print("Biases and Weights:")            for i in xrange(0, len(self.weights)):                print("For Layer "+str(i+1)+" with Format ("+str(self.neuron_list[i])+","+str(self.neuron_list[i+1])+")")                print("Biase:\n"+str(self.biases[i]))                print("Weight:\n"+str(self.weights[i]))        else:            print("Matrix is not initialized!!!")        print("Plot is set to "+str(self.is_show_plot))    def save_best_network(self):        if self.is_best_network_saved == True:            if self.last_best_network["lowest_error"] <= self.last_error:                print(self.error_network_not_better)                return -1        else:            self.is_best_network_saved = True        lbn = self.last_best_network        lbn["neuron_list"] = deepcopy(self.neuron_list)        lbn["biases"] = deepcopy(self.biases)        lbn["weights"] = deepcopy(self.weights)        lbn["inputs"] = deepcopy(self.inputs)        lbn["targets"] = deepcopy(self.targets)        lbn["learning_rate"] = self.learning_rate        lbn["lowest_error"] = self.last_error        print("Network was now saved!")        return 0    def load_last_network(self):        if self.is_best_network_saved == False:            print(error_network_not_better)            return -1        lbn = self.last_best_network        self.neuron_list = deepcopy(lbn["neuron_list"])        self.biases = deepcopy(lbn["biases"])        self.weights = deepcopy(lbn["weights"])        self.inputs = deepcopy(lbn["inputs"])        self.targets = deepcopy(lbn["targets"])        self.learning_rate = lbn["learning_rate"]        self.last_error = lbn["lowest_error"]        print("Network was now loaded!")        return 0    def reset_saved_networks(self):        self.last_best_network = deepcopy(self.last_best_network_default)        self.is_best_network_saved = False        print("Reseted best network!")    def show_error_plot(self, error_list, width=100):        p_x = [i for i in xrange(0, len(self.error_list))]        p_y = [e for e in error_list]        # length = len(error_list)        # index = int(length / width)        # for i in xrange(0, index):        #     # plt.figure()        #     plt.plot(p_x[width*i:width*(i+1)], p_y[width*i:width*(i+1)])        # # for        # if length % 100 != 0:        #     plt.plot(p_x[width*index:], p_y[width*index:])        # # if        plt.plot(p_x, p_y)        plt.yscale("log")        plt.show(block = False)    # def show_error_plot    def show_own_error_plot(self):        self.show_error_plot(self.error_list)    # def show_error_plot    def show_outputs_targets_plot_points(self, inputs, targets, should_show=True):        # Get the network biases and weights        biases = self.biases        weights = self.weights        # Calculate the output of this network        outputs = self.calculate_forward_many(inputs, biases, weights)        # Modifie and concat the List of outputs and targets        # and x-Axis as the output values and the targets as y-Axis        outputs = [np.transpose(o).tolist()[0] for o in outputs]        targets = [np.transpose(t).tolist()[0] for t in targets]        # Set the new Lists        output_values = [] # x-Points        target_values = [] # y-Points        for (o, t) in zip(outputs, targets):            output_values += o            target_values += t        # for        changing_factor = 1 / float(len(output_values))        # Add to every value a change factor for better plotting        for i in xrange(0, len(output_values)):            output_values[i] += float(i) * changing_factor            target_values[i] += float(i) * changing_factor        # for        xy = []        for (o, t) in zip(output_values, target_values):            xy.append((o, t))        # for        sorted(xy, key = lambda x : x[0])        output_values = []        target_values = []        for (o, t) in xy:            output_values.append(o)            target_values.append(t)        # for                x = np.array([0.0] + output_values)        y = np.array([0.0] + target_values)        # print(str(output_values))        # print(str(target_values))                x = x[:, np.newaxis]        a, _, _, _ = np.linalg.lstsq(x, [0.0] + target_values)        index_min = 0        index_max = 0        val_min = 10.        val_max = -10.        for (i, px, py) in zip(xrange(0, len(x)), x, y):            val = py - px * a            if val_min > val:                val_min = val                index_min = i            # if            if val > val_max:                val_max = val                index_max = i            # if        # for        # down range        line1_x = [0.0, 2.0]        d1 = y[index_min] - a * x[index_min]        line1_y = [d1, 2.0 * a + d1]        # up range        line2_x = [0.0, 2.0]        d2 = y[index_max] - a * x[index_max]        line2_y = [d2, 2.0 * a + d2]        print("a = "+str(a))        # print("d_max - d_min = "+str(line2_y[0]-line1_y[0]))        # print("atan(a) = "+str(math.atan(a[0])))        # print("cos(atan(a)) = "+str(math.cos(math.atan(a[0]))))        print("d_normal = "+str((line2_y[0]-line1_y[0])*math.cos(math.atan(a[0]))))                # Now plot the graph        figure = plt.figure()        axes = figure.add_subplot(111, aspect="equal")#plt.gca()        axes.set_xlim([0., 2.])        axes.set_ylim([0., 2.])        plt.figtext(0.6, 0.2, "a = "+str(round(a[0], 5))+"\nd_norm = "+str(round([(line2_y[0]-line1_y[0])*math.cos(math.atan(a[0]))][0][0], 5)))                plt.plot(output_values, target_values, "ro")        plt.plot([0., 2.], [0., a*2.], "r-")        plt.plot([0., 2.], [0., 2.], "k-")        # plt.plot(x, a*x, "r-")        plt.xlabel("Outputs + Offset")        plt.ylabel("Targets + Offset")        plt.plot(line1_x, line1_y, "g-")        plt.plot(line2_x, line2_y, "g-")        plt.grid(True)        if should_show:            plt.show(block = False)        return figure    # def show_outputs_targets_plot    def show_own_outputs_targets_plot_points(self):        # The the network inputs and targets        inputs = self.inputs        targets = self.targets        self.show_outputs_targets_plot(inputs, targets)    # def show_outputs_targets_plot_points    def show_outputs_targets_plot_bars(self):        inputs = self.inputs        targets = self.targets    def show_plot(self, error_lists):        print("comes in!")        for el in error_lists:            p_x = [i for i in xrange(0, len(el))]            p_y = [e for e in el]            plt.plot(p_x, p_y)        # for        plt.yscale("log")        plt.show()	def picture_list_to_matrix(img_1d_array, width, height, scale):	    w, h = width, height	    # scale = 20	    print("scale factor in pucture list to matrix = "+str(scale))	    data = np.zeros((scale*h, scale*w, 3), dtype=np.uint8)	        	    img_2d_array = [list(img_1d_array[w*i:w*(i+1)]) for i in xrange(0, h)]	    # print("img_2d_array = "+str(img_2d_array))	    # print("img_array = "+str(img_array))	    # print("len y = "+str(len(img_2d_array))+"    len x = "+str(len(img_2d_array[0])))	    # print("img_2d_array =\n"+str(img_2d_array))	    for y in xrange(0, h):	        for x in xrange(0, w):	            # print("x = "+str(x)+"   y = "+str(y))	            color_value = int(float(img_2d_array[y][x]) * 255.)	            color = (color_value, color_value, color_value)	            for ys in xrange(0, scale):	                for xs in xrange(0, scale):	                    data[scale*y+ys, scale*x+xs] = color	                # for	            # for	        # for	    # for	    return data	# def pixels_list_to_2d_list	def show_picture(input_array, value, index, width, height, scale):	    # w, h = 28, 28	    # w, h = 14, 14	    input_array = self.inputs[index]	    width = math.sqrt(len(input_array))	    print("scale factor in show = "+str(scale))	    data = picture_list_to_matrix(img_1d_array, width, height, scale)	    img = Image.fromarray(data, "RGB")	    img.show(title="Number "+str(value))	    # img.save(picture_directory+"train_"+str(index)+"_num_"+str(list_pixels[2][index])+".png")	    print("The Digit of picture #"+str(index)+" is: "+str(value))	# def sace_pictures_in_directory    def get_random_matrix(self, rows, columns, min_val, max_val, roundCom):        matrix = []        if self.is_decimal:            i = 0            while i < rows:                tmp = []                j = 0                while j < columns:                    tmp.append(dec(round(uniform(min_val, max_val), roundCom)))                    j += 1                matrix.append(tmp)                i += 1        else:            i = 0            while i < rows:                tmp = []                j = 0                while j < columns:                    tmp.append(round(uniform(min_val, max_val), roundCom))                    j += 1                matrix.append(tmp)                i += 1        # if        return matrix    # def get_random_matrix    def get_input_layer(self):        return self.inputs_amount    def get_output_layer(self):        return self.outputs_amount    def get_neuron_list(self):        return self.neuron_list    def get_biases(self):        return self.biases    def get_weights(self):        return self.weights    def get_learning_rate(self):        return self.learning_rate    def get_random_biases_list(self, neuron_list, nmin = -3, nmax = 3, ncommas = 5):        nl = neuron_list        biases = [np.array(self.get_random_matrix(1, nl[i + 1], nmin, nmax, ncommas)) for i in xrange(0, len(nl) - 1)]        return biases    def get_random_weights_list(self, neuron_list, nmin = -3, nmax = 3, ncommas = 5):        nl = neuron_list        weights = [np.array(self.get_random_matrix(nl[i], nl[i + 1], nmin, nmax, ncommas)) for i in xrange(0, len(nl) - 1)]        return weights    def get_zero_biases_weights(self):        biases = [np.zeros((i, )) for i in self.neuron_list[1:]]        weights = [np.zeros((i, j)) for (i, j) in zip(self.neuron_list[:-1], self.neuron_list[1:])]                return biases, weights    # def get_zero_biases_weights    def get_random_neurons_weights(self, neuron_list, nmin = -3, nmax = 3, ncommas = 5):        nl = neuron_list        biases = [np.array(self.get_random_matrix(1, nl[i + 1], nmin, nmax, ncommas)) for i in xrange(0, len(nl) - 1)]        weights = [np.array(self.get_random_matrix(nl[i], nl[i + 1], nmin, nmax, ncommas)) for i in xrange(0, len(nl) - 1)]        return biases, weights    def get_name_of_neural_network(self):        return self.neural_name    def get_hidden_function(self):        return self.hidden_function    def get_type_learning(self):        return self.type_learning    def get_network_type(self):        return self.network_type    def get_max_min_etha(self):        return self.max_etha, self.min_etha    def get_adapt_speed(self):        return self.adapt_learn_pos, self.adapt_learn_neg    # Setter    def set_neuron_list(self, inputs):# for input, hidden and output layer simultaneously        self.matrix_init = False        self.neuron_list = inputs    def set_biases(self, biases):        self.biases = biases    def set_weights(self, weights):        self.weights = weights    def get_network(self):        return [self.neuron_list, self.biases, self.weights]    def set_network(self, network):        self.neuronal_list = network[0]        self.biases = network[1]        self.weights = network[2]    def set_max_min_etha(self, max_etha, min_etha):        self.max_etha = max_etha        self.min_etha = min_etha    def set_hidden_function(self, hidden_function):        if hidden_function == "sigmoid" or hidden_function == "tanh":            self.hidden_function = hidden_function        else:            self.hidden_function = "sigmoid"    def set_name_of_neural_network(self, neural_name):        self.neural_name = neural_name    def set_type_of_learning(self, type_learning):        self.type_learning = type_learning        if type_learning == "sgd":            self.backprop_style = "BACKPROP_SGD"        elif type_learning == "bgd":            self.backprop_style = "BACKPROP_BGD"        else:            self.backprop_style = "BACKPROP_SGD"    def set_is_adaptive(self, is_adaptive):            self.is_adaptive = is_adaptive    def set_is_no_random(self, is_no_random):        self.is_no_random = is_no_random    def set_network_type(self, network_type):        if network_type == "regression" or network_type == "classifier":            self.network_type = network_type        else:            self.network_type = "regression"    def set_adapt_speed(self, pos, neg):        self.adapt_learn_pos = pos        self.adapt_learn_neg = neg    def init_random_weights(self):        self.matrix_init = True        nl = self.neuron_list        weights = []        biases = []        for i in xrange(0, len(nl) - 1):            value = 0.5 / nl[i] # np.sqrt(nl[i]) # 0.5 is for tanh e.g.            biases.append((np.random.random((nl[i+1], ))-0.5)*2*value)            weights.append((np.random.random((nl[i], nl[i+1]))-0.5)*2*value)        self.biases = biases        self.weights = weights        self.reset_saved_networks()        self.weights_init = True        print("Created random biases and weights for "+str(self.neuron_list)+" neuronal list")    def calculate_forward_many(self, x, bs, ws):        ones = np.ones((x.shape[0], 1))        for b, w in zip(bs, ws):            x = self.sigmoid(np.dot(np.hstack((ones, x)), np.vstack((b, w))))        return x    def backpropagation(self, start_input, biases, weights, targets, etha, func=None, funcdev=None):        # Feed forward        xs = []        ys = [start_input]        # Set all functions for calculations        if self.is_decimal:            _sigmoid = self.sigmoid_decimal            _sigmoid_invert = self.sigmoid_invert_decimal            _tanh = self.tanh_decimal            _tanh_invert = self.tanh_decimal_invert        else:            _sigmoid = self.sigmoid            _sigmoid_invert = self.sigmoid_invert            _tanh = self.tanh            _tanh_invert = self.tanh_invert        if self.hidden_function == "tanh":            hidden_func = _tanh            hidden_func_deriv = _tanh_invert        else:            hidden_func = _sigmoid            hidden_func_deriv = _sigmoid_invert        # Calculate forward and save all activations in xs and outputs in ys        y = start_input        for b, w in zip(biases[:-1], weights[:-1]): #xrange(0, len(weights)):            a = np.dot(w.transpose(), y) + b            y = hidden_func(a)            xs.append(a)            ys.append(y)        a = np.dot(weights[-1].transpose(), y)+biases[-1]        # TODO implement soft max        if self.network_type == "classifier":            y = (lambda x: x / np.sum(x))(np.exp(a))        else:            y = _sigmoid(a)        xs.append(a)        ys.append(y)        # Init zero arrays        bs = [0 for b in biases]        ws = [0 for w in weights]        # Backward error correction        d = etha*np.subtract(ys[-1], targets) * _sigmoid_invert(xs[-1])        diff = np.outer(np.hstack(([1], ys[-2])), d)        bs[-1] = diff[0, :]        ws[-1] = diff[1:, :]        for i in xrange(2, len(weights) + 1):            d = np.dot(weights[-i + 1], d) * hidden_func_deriv(xs[-i])            diff = np.outer(np.hstack(([1], ys[-i-1])), d)            bs[-i] = diff[0, :]            ws[-i] = diff[1:, :]        return (bs, ws)    def backprop_many(self, x, bs, ws, t, etha):        xs = []        ys = [x]        y = x        rows = x.shape[0]        ones = np.ones((rows, 1))        bws = [np.vstack((b, w)) for b, w in zip(bs, ws)]        # feed forward        for bw in bws[:-1]:            y = np.dot(np.hstack((ones, y)), bw)            xs.append(y.copy())            y = self.sigmoid(y)            ys.append(y.copy())        bw = bws[-1]        y = np.dot(np.hstack((ones, y)), bw)        xs.append(y.copy())        # print("self.network_type = {}".format(self.network_type))        if self.network_type == "classifier":            ys.append((lambda x: x / np.sum(x, axis=1).reshape((x.shape[0], 1)))(np.exp(y)))        else:            ys.append(self.sigmoid(y))        # backpropagation        d = (ys[-1]-t)*self.sigmoid_invert(xs[-1])        bwsd = [0 for _ in xrange(len(ws))]        bwsd[-1] = np.hstack((ones, ys[-2])).T.dot(d)        for i in xrange(2, len(ws)+1):            d = np.dot(d, ws[-i+1].transpose())*self.sigmoid_invert(xs[-i])            bwsd[-i] = np.hstack((ones, ys[-i-1])).T.dot(d)        bsdt = [bwsdi[0, :] for bwsdi in bwsd]        wsdt = [bwsdi[1:, :] for bwsdi in bwsd]        return bsdt, wsdt    def get_partition_list(self, amount, partitions):        whole, rest = amount // partitions, amount % partitions        return reduce(lambda a, b: a+[a[-1]+b], [whole+1 for _ in xrange(rest)]+[whole for _ in xrange(partitions-rest)], [0])    def calc_forward_backprop_procs(self, input_queue, output_queue):        cecf_vectorize = np.vectorize(lambda targ, pred: -np.log(1-pred) if targ == 0 else -np.log(pred))        inp_tr, targ_tr, inp_tst, targ_tst = input_queue.get()        while True:            command, arguments = input_queue.get()            if command == "FINISHED":                return            elif command == "BACKPROP_SGD":                bs, ws, etha = arguments                bs = deepcopy(bs)                ws = deepcopy(ws)                bsdtsum = [np.zeros(b.shape) for b in bs]                wsdtsum = [np.zeros(w.shape) for w in ws]                parts = 30                partition = self.get_partition_list(inp_tr.shape[0], parts)                # for _ in xrange(5):                perms = np.random.permutation(np.arange(0, inp_tr.shape[0]))                inp_tr_perms, targ_tr_perms = inp_tr[perms], targ_tr[perms]                for p1, p2 in zip(partition[:-1], partition[1:]):                    bsdt, wsdt = self.backprop_many(inp_tr_perms[p1:p2, :], bs, ws, targ_tr_perms[p1:p2, :], etha)                    for bsi, wsi, bsdtsumi, wsdtsumi, bsdti, wsdti in zip(bs, ws, bsdtsum, wsdtsum, bsdt, wsdt):                        bsi -= bsdti# / (p2-p1)                        wsi -= wsdti# / (p2-p1)                        bsdtsumi += bsdti# / (p2-p1)                        wsdtsumi += wsdti# / (p2-p1)                output_queue.put((bsdtsum, wsdtsum))            elif command == "BACKPROP_BGD":                bs, ws, etha = arguments                bsdt, wsdt = self.backprop_many(inp_tr, bs, ws, targ_tr, etha)                output_queue.put((bsdt, wsdt))            elif command == "BACKPROP_BGD_DEBUG":                bs, ws, etha, file_name = arguments                bsdt, wsdt = self.backprop_many_debug(inp_tr, bs, ws, targ_tr, etha, file_name)                output_queue.put((bsdt, wsdt))            elif command == "CALCFORWARD":                bs, ws = arguments                out_tr = self.calculate_forward_many(inp_tr, bs, ws)                out_tst = self.calculate_forward_many(inp_tst, bs, ws)                sum_tr = np.sum(np.sum((out_tr - targ_tr)**2, axis=1))                sum_tst = np.sum(np.sum((out_tst - targ_tst)**2, axis=1))                sum_tr_cecf = np.sum(cecf_vectorize(targ_tr, out_tr))                sum_tst_cecf = np.sum(cecf_vectorize(targ_tst, out_tst))                train_true = np.sum(np.argmax(out_tr, axis=1) == np.argmax(targ_tr, axis=1))                test_true = np.sum(np.argmax(out_tst, axis=1) == np.argmax(targ_tst, axis=1))                output_queue.put((sum_tr, sum_tst, sum_tr_cecf, sum_tst_cecf, train_true, test_true))            elif command == "CONFUSION":                bs, ws, classes = arguments                out_tr = self.calculate_forward_many(inp_tr, bs, ws)                out_tst = self.calculate_forward_many(inp_tst, bs, ws)                out_tr_argmax = np.argmax(out_tr, axis=1)                out_tst_argmax = np.argmax(out_tst, axis=1)                targ_tr_argmax = np.argmax(targ_tr, axis=1)                targ_tst_argmax = np.argmax(targ_tst, axis=1)                confusion_tr = np.zeros((classes, classes), dtype=np.uint32)                confusion_tst = np.zeros((classes, classes), dtype=np.uint32)                for o, t in zip(out_tr_argmax, targ_tr_argmax):                    confusion_tr[(o, t)] += 1                for o, t in zip(out_tst_argmax, targ_tst_argmax):                    confusion_tst[(o, t)] += 1                output_queue.put((confusion_tr, confusion_tst))    def confusion_matrix_better_multiprocess(self, network, inputs_train, targets_train, inputs_test, targets_test, copy=False):        """        This will improve the given network with adaptive learning and will also give the error list back (for train and test).        Also the trainings rate etha per iteration.        :param network: yuhuu!!!        :param inputs_train:        :param targets_train:        :param inputs_test:        :param targets_test:        :param copy:        :return:        """        cores = mp.cpu_count()        amount_tr = inputs_train.shape[0]        amount_tst = inputs_test.shape[0]        partition_tr = self.get_partition_list(amount_tr, cores)        partition_tst = self.get_partition_list(amount_tst, cores)        input_queues = [mp.Queue() for _ in xrange(cores)]        output_queues = [mp.Queue() for _ in xrange(cores)]        procs = [mp.Process(target=self.calc_forward_backprop_procs, args=(input_queues[i], output_queues[i])) for i in xrange(cores)]        for i in xrange(cores):            input_queues[i].put((inputs_train[partition_tr[i]:partition_tr[i+1]], targets_train[partition_tr[i]:partition_tr[i+1]],                                 inputs_test[partition_tst[i]:partition_tst[i+1]], targets_test[partition_tst[i]:partition_tst[i+1]]))        for proc in procs: proc.start()        nl, bs, ws = network        if copy:            nl, bs, ws = deepcopy([nl, bs, ws])        ## Multiprocessing calc forward ##        classes = targets_train.shape[1]        confusion_tr = np.zeros((classes, classes), dtype=np.uint32)        confusion_tst = np.zeros((classes, classes), dtype=np.uint32)        for input_queue in input_queues: input_queue.put(("CONFUSION", (bs, ws, classes)))        for output_queue in output_queues:            get = output_queue.get()            confusion_tr += get[0]; confusion_tst += get[1]        for input_queue in input_queues: input_queue.put(("FINISHED", 0))        for proc in procs: proc.join()        return confusion_tr, confusion_tst    def calc_forward_better_multiprocess(self, network, inputs_train, targets_train, inputs_test, targets_test, copy=False):        """        Calculate multiprocess        :param network: yuhuu!!!        :param inputs_train:        :param targets_train:        :param inputs_test:        :param targets_test:        :param copy:        :return:        """        cores = mp.cpu_count()        amount_tr = inputs_train.shape[0]        amount_tst = inputs_test.shape[0]        partition_tr = self.get_partition_list(amount_tr, cores)        partition_tst = self.get_partition_list(amount_tst, cores)        input_queues = [mp.Queue() for _ in xrange(cores)]        output_queues = [mp.Queue() for _ in xrange(cores)]        procs = [mp.Process(target=self.calc_forward_backprop_procs, args=(input_queues[i], output_queues[i])) for i in xrange(cores)]        for i in xrange(cores):            input_queues[i].put((inputs_train[partition_tr[i]:partition_tr[i+1]], targets_train[partition_tr[i]:partition_tr[i+1]],                                 inputs_test[partition_tst[i]:partition_tst[i+1]], targets_test[partition_tst[i]:partition_tst[i+1]]))        for proc in procs: proc.start()        nl, bs, ws = network        if copy:            nl, bs, ws = deepcopy([nl, bs, ws])        ## Multiprocessing calc forward ##        for input_queue in input_queues: input_queue.put(("CALCFORWARD", (bs, ws)))        sum_error_tr = 0.        sum_error_tst = 0.        sum_error_tr_cecf = 0.        sum_error_tst_cecf = 0.        class_train_true = 0        class_test_true = 0        for output_queue in output_queues:            get = output_queue.get()            sum_error_tr += get[0]; sum_error_tst += get[1]            sum_error_tr_cecf += get[2]; sum_error_tst_cecf += get[3]            class_train_true += get[4]; class_test_true += get[5]        error_train = sum_error_tr / amount_tr        error_test = sum_error_tst / amount_tst        for input_queue in input_queues: input_queue.put(("FINISHED", 0))        for proc in procs: proc.join()        return error_train, error_test, sum_error_tr_cecf, sum_error_tst_cecf, class_train_true, class_test_true    def improve_network_multiprocess(self, network, inputs_train, targets_train, inputs_test, targets_test,                                            iterations, start_etha, copy=False, withrandom=False, start_iter=0):        """        This will improve the given network with adaptive learning and will also give the error list back (for train and test).        Also the trainings rate etha per iteration.        :param network: yuhuu!!!        :param inputs_train:        :param targets_train:        :param inputs_test:        :param targets_test:        :param iterations:        :param start_etha:        :param copy:        :return:        """        cores = mp.cpu_count()        amount_tr = inputs_train.shape[0]        amount_tst = inputs_test.shape[0]        partition_tr = self.get_partition_list(amount_tr, cores)        partition_tst = self.get_partition_list(amount_tst, cores)        input_queues = [mp.Queue() for _ in xrange(cores)]        output_queues = [mp.Queue() for _ in xrange(cores)]        procs = [mp.Process(target=self.calc_forward_backprop_procs, args=(input_queues[i], output_queues[i])) for i in xrange(cores)]        for i in xrange(cores):            input_queues[i].put((inputs_train[partition_tr[i]:partition_tr[i+1]], targets_train[partition_tr[i]:partition_tr[i+1]],                                 inputs_test[partition_tst[i]:partition_tst[i+1]], targets_test[partition_tst[i]:partition_tst[i+1]]))        for proc in procs: proc.start()        etha = start_etha        print("etha = "+str(etha))        nl, bs, ws = network        if copy:            nl, bs, ws = deepcopy([nl, bs, ws])        ## Multiprocessing calc forward ##        for input_queue in input_queues: input_queue.put(("CALCFORWARD", (bs, ws)))        sum_error_tr = 0.        sum_error_tst = 0.        sum_error_tr_cecf = 0.        sum_error_tst_cecf = 0.        class_train_true = 0        class_test_true = 0        for output_queue in output_queues:            get = output_queue.get()            sum_error_tr += get[0]; sum_error_tst += get[1]            sum_error_tr_cecf += get[2]; sum_error_tst_cecf += get[3]            class_train_true += get[4]; class_test_true += get[5]        errors_train = [np.sqrt(sum_error_tr / amount_tr)]        errors_test = [np.sqrt(sum_error_tst / amount_tst)]        errors_train_cecf = [sum_error_tr_cecf]        errors_test_cecf = [sum_error_tst_cecf]        classes_train_true = [class_train_true]        classes_test_true = [class_test_true]        max_wsmods = [[] for _ in nl[:-1]]        max_bsmods = [[] for _ in nl[:-1]]        min_wsmods = [[] for _ in nl[:-1]]        min_bsmods = [[] for _ in nl[:-1]]        print("iteration = {:4}   etha = {:03.9f}   err_train = {:0.9f}".format(0, etha, errors_train[0]))        etha_per_iteration = [[etha], [0]]        i = start_iter        iterations += start_iter        max_etha = self.max_etha        min_etha = self.min_etha        adapt_learn_pos = self.adapt_learn_pos        adapt_learn_neg = self.adapt_learn_neg        backprop_style = self.backprop_style        is_no_random = self.is_no_random        is_adaptive = self.is_adaptive        fails = 0        norm_etha = 0.01        gamma = 0.3        is_bigger = False        print("max_etha = {}".format(max_etha))        print("min_etha = {}".format(min_etha))        max_fails = 10        fails_no_random = 5        while i < iterations:            start_time = time.time()            bsmod, wsmod = deepcopy(bs), deepcopy(ws)            # if (withrandom or is_bigger) and is_no_random and fails < 1:# and fails < max_fails - fails_no_random:            #     bsmod = [bi+np.random.normal(0, (norm_etha / (1.+i)**gamma), (bi.shape)) for bi in bsmod]            #     wsmod = [wi+np.random.normal(0, (norm_etha / (1.+i)**gamma), (wi.shape)) for wi in wsmod]            ## Multiprocessing backpropagation ##            for input_queue in input_queues: input_queue.put((backprop_style, (bsmod, wsmod, etha)))            bsdif, wsdif = self.get_zero_biases_weights()            for output_queue in output_queues:                get = output_queue.get()                bsdt = get[0]                wsdt = get[1]                for bsdifi, wsdifi, bsdti, wsdti in zip(bsdif, wsdif, bsdt, wsdt):                    bsdifi += bsdti                    wsdifi += wsdti            while True:                print("etha = {}".format(etha))                bsmod, wsmod = deepcopy(bs), deepcopy(ws)                for bsmodi, wsmodi, bsdifi, wsdifi in zip(bsmod, wsmod, bsdif, wsdif):                    bsmodi += etha * bsdifi / len(inputs_train)                    wsmodi += etha * wsdifi / len(inputs_train)                ## Multiprocessing calc forward ##                for input_queue in input_queues: input_queue.put(("CALCFORWARD", (bsmod, wsmod)))                sum_error_tr = 0.                sum_error_tst = 0.                sum_error_tr_cecf = 0.                sum_error_tst_cecf = 0.                class_train_true = 0                class_test_true = 0                for output_queue in output_queues:                    get = output_queue.get()                    sum_error_tr += get[0]; sum_error_tst += get[1]                    sum_error_tr_cecf += get[2]; sum_error_tst_cecf += get[3]                    class_train_true += get[4]; class_test_true += get[5]                error_train = np.sqrt(sum_error_tr / amount_tr)                error_test = np.sqrt(sum_error_tst / amount_tst)                error_train_cecf = sum_error_tr_cecf / amount_tr                error_test_cecf = sum_error_tst_cecf / amount_tst                if errors_train_cecf[-1] > error_train_cecf:                    etha *= adapt_learn_pos                    break                etha *= adapt_learn_neg                if etha < min_etha:                    etha = min_etha                    break                # if errors_train_cecf[-1] >= error_train_cecf: # errors_train[-1] > error_train: # or (error_train - errors_train[-1]) < 0.00001:                #     is_bigger = False                #     if is_adaptive:                #         etha *= adapt_learn_pos                #     if etha > max_etha:                #         etha = max_etha                #     fails = 0                # else:                #     is_bigger = True                #     fails += 1                #     if is_adaptive:                #         etha *= adapt_learn_neg                #     if etha < min_etha:                #         etha = min_etha            etha_per_iteration[0].append(etha)            etha_per_iteration[1].append(i)            taken_time = time.time() - start_time            max_wsmod, max_bsmod = map(np.max, wsmod), map(np.max, bsmod)            min_wsmod, min_bsmod = map(np.min, wsmod), map(np.min, bsmod)            print("iteration = {:5}".format(i)+                  "   etha = {:03.9f}".format(etha)+                  "   err_train = {:0.9f}".format(error_train)+                  "   err_train_cecf = {:0.9f}".format(error_train_cecf)+                  "   is Bigger? {:6}".format(str(bool(is_bigger)))+                  "   predict_train_true = {:5}".format(class_train_true)+                  "   taken_time = {:3.3f}".format(taken_time)+                  "\nmax weights: {}, max biases: {}".format(map(np.max, wsmod), map(np.max, bsmod))+                  "\nmin weights: {}, min biases: {}".format(map(np.min, wsmod), map(np.min, bsmod))+"\n")            # if is_bigger:# and fails < max_fails:            #     if fails > 1:                #         if is_adaptive:                #             etha *= adapt_learn_neg                #         if etha < min_etha:                #             etha = min_etha                # if fails < max_fails - fails_no_random:                #     continue            if fails > 1:                fails = 0            if fails > 0:                continue            # else:            #     print("set etha *= adapt_pos**fials_no_random")            #     etha *= adapt_learn_pos**(fails_no_random*5)            #     print("etha = {}".format(etha))            #     fails = 0            errors_train.append(error_train)            errors_test.append(error_test)            errors_train_cecf.append(error_train_cecf)            errors_test_cecf.append(error_test_cecf)            classes_train_true.append(class_train_true)            classes_test_true.append(class_test_true)            append_lists = lambda x, y: map(lambda (x, y): x.append(y), zip(x, y))            append_lists(max_wsmods, max_wsmod)            append_lists(max_bsmods, max_bsmod)            append_lists(min_wsmods, min_wsmod)            append_lists(min_bsmods, min_bsmod)            bs, ws = bsmod, wsmod            i += 1        for input_queue in input_queues: input_queue.put(("FINISHED", 0))        for proc in procs: proc.join()        return [nl, bs, ws], errors_train, errors_test, errors_train_cecf, errors_test_cecf, etha_per_iteration, \               classes_train_true, classes_test_true, max_wsmods, max_bsmods, min_wsmods, min_bsmods